{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwucHCkUOSLnxM27pkSpOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaForziati/DS3_coderhouse/blob/main/trabajo_final_redes_neuronales_ForziatiGangi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clasificador de flores**"
      ],
      "metadata": {
        "id": "D3Ip4sbKYrPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El presente proyecto busca la creación de un sistema que sea capaz de clasificar diferentes flores.\n",
        "\n",
        "Se toma el dataset \"Flowers102\" de los datasets disponibles en PyTorch."
      ],
      "metadata": {
        "id": "XCfAvvr2Yx0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# librerias"
      ],
      "metadata": {
        "id": "lxhVobnVZDaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a2ssvA5WtZ5E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from IPython import display\n",
        "from torchvision import transforms\n",
        "from torch.utils import data\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cargar dataset"
      ],
      "metadata": {
        "id": "eFeIyT77ZFUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cargar el dataset que se extrae los datasets de torchvision."
      ],
      "metadata": {
        "id": "GWbP8XgIZnJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funcion que carga el dataset\n",
        "def load_data_flowers102(batch_size, resize=28):\n",
        "    # definir transformacion. Todas las imagenes tendran un tamaño de 28x28\n",
        "    trans = transforms.Compose([\n",
        "        transforms.Resize((resize, resize)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # cargar los datasets de entrenamiento y prueba\n",
        "    flowers_train = torchvision.datasets.Flowers102(\n",
        "        root=\"../data\", split=\"train\", transform=trans, download=True)\n",
        "    flowers_test = torchvision.datasets.Flowers102(\n",
        "        root=\"../data\", split=\"test\", transform=trans, download=True)\n",
        "\n",
        "    # crear los dataloaders\n",
        "    train_loader = data.DataLoader(flowers_train, batch_size,\n",
        "                                 shuffle=True, num_workers=1)\n",
        "    test_loader = data.DataLoader(flowers_test, batch_size,\n",
        "                                shuffle=False, num_workers=1)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "IIC-ZHuH3LVa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definir el tamaño del batch\n",
        "batch_size = 256\n",
        "\n",
        "# cargar los dataset y guardarlos\n",
        "train_iter, test_iter = load_data_flowers102(batch_size)"
      ],
      "metadata": {
        "id": "vnG228RFt5aC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero, definimos el tamaño de imagen y número de clases\n",
        "image_size = 28  # Mantenemos 28x28 por simplicidad\n",
        "num_classes = 102  # Flowers102 tiene 102 clases\n",
        "input_size = image_size * image_size * 3  # * 3 porque son imágenes RGB"
      ],
      "metadata": {
        "id": "44jxraA75wHj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Fully Connected"
      ],
      "metadata": {
        "id": "SdFVZOsrg9kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# crear la red neuronal\n",
        "net = nn.Sequential( # cada capa se aplica una detras de la otra\n",
        "    nn.Flatten(), # aplanar las imagenes\n",
        "    nn.Linear(input_size, 256),  # Capa inicial, fully connected\n",
        "    nn.ReLU(),                   # funcion de activacion ReLU\n",
        "    nn.Linear(256, 128),         # Otra capa fully connected que reduce las caracteristicas de 256 -> 128\n",
        "    nn.ReLU(),                   # Otra función de activación ReLU\n",
        "    nn.Linear(128, num_classes)  # Capa de salida\n",
        ")\n",
        "\n",
        "# pesos de las capas fully connected\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnsFv4vB4e9v",
        "outputId": "ecfd4b07-1b28-491c-e60d-ff81c2b1a882"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten(start_dim=1, end_dim=-1)\n",
              "  (1): Linear(in_features=2352, out_features=256, bias=True)\n",
              "  (2): ReLU()\n",
              "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (4): ReLU()\n",
              "  (5): Linear(in_features=128, out_features=102, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros de entrenamiento\n",
        "# se define el tamaño del batch, la tasa de aprendizaje y el numero de epochs\n",
        "batch_size, lr, num_epochs = 256, 0.1, 10\n",
        "# funcion de perdida\n",
        "loss = nn.CrossEntropyLoss(reduction='none')\n",
        "# optimizador\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "iwnsYbj-4RjP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de precisión\n",
        "def accuracy(y_hat, y):\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())"
      ],
      "metadata": {
        "id": "InDVR5Ni4Wln"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucle de entrenamiento\n",
        "for epoch in range(num_epochs):\n",
        "    # variables para almacenar la perdida, el numero de muestras, la precision del entrenamiento y la precision de la prueba\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    Acc = 0.0\n",
        "    TestAcc = 0.0\n",
        "    TestN = 0\n",
        "\n",
        "    # Entrenamiento\n",
        "    net.train()\n",
        "    for X, y in train_iter:\n",
        "        # obtener las predicciones del modelo para las entradas X\n",
        "        y_hat = net(X)\n",
        "        # calculo de la perdida\n",
        "        l = loss(y_hat, y)\n",
        "        # limpiar los gradientes\n",
        "        trainer.zero_grad()\n",
        "        # calcular los gradientes de la perdida media\n",
        "        l.mean().backward()\n",
        "        # actualizar los parametros\n",
        "        trainer.step()\n",
        "        # sumar la perdida total\n",
        "        L += l.sum()\n",
        "        # sumar el numero de muestras\n",
        "        N += l.numel()\n",
        "        Acc += accuracy(y_hat, y)\n",
        "\n",
        "    # Evaluación\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_iter:\n",
        "            y_hat = net(X)\n",
        "            TestN += y.numel()\n",
        "            TestAcc += accuracy(y_hat, y)\n",
        "\n",
        "    print(f'epoch {epoch + 1}, loss {(L/N):f}, '\n",
        "          f'train accuracy {(Acc/N):f}, test accuracy {(TestAcc/TestN):f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUA_-TvH4p2t",
        "outputId": "48942184-6def-4732-a7ca-6fa0123a2f1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 4.626585, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 2, loss 4.626545, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 3, loss 4.626492, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 4, loss 4.626464, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 5, loss 4.626407, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 6, loss 4.626363, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 7, loss 4.626310, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 8, loss 4.626290, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 9, loss 4.626254, train accuracy 0.009804, test accuracy 0.005529\n",
            "epoch 10, loss 4.626193, train accuracy 0.009804, test accuracy 0.005529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que los resultados obtenidos en base a la red construida son extremadamente bajos. El modelo no logra generalizar, dando una precisión muy baja. Por cada epoca la funcion de perdida se mantiene casi igual.\n",
        "\n",
        "Es necesario implementar una arquitectura diferente para lograr resolver el presente problema."
      ],
      "metadata": {
        "id": "89N0tvYBgP2n"
      }
    }
  ]
}